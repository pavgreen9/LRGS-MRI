# Cluster Correction

The analysis carried out a statistical test for every vertex we had. Given the numbers, this can lead to a large number of false positives. To control for the number of false positives, we do a cluster correction.

Instead of calculating the sig of each vertex individually and doing a Bonferroni Correction - sig/number of test i.e. 0.05/vertex number - we do a cluster correction which takes a cluster of vertex and tests it for significance. The rationale behind it lies in the fact that vertex can often be grouped together i.e. if a vertex has a high volume, there’s a very good probability that the surrounding vertices would also have high volumes. We can then set a threshold for clusters that reach a certain `z-value` .

To illustrate this, let’s use an fMRI coronal slice from the incongruent-congruent group-level contrast (replace vertex with voxels). Notice how the brighter colors can be grouped into distinct clusters (in our case high volumes/thickness); if we tilt this slice into a three-dimensional view, we can see how clusters of voxels clump together into what looks like mountain ranges.

Seen from this angle, the height of an individual voxel is determined by its z-value: Higher z-values correspond to higher peaks. The threshold that we apply is a cross-section through the mountains at a certain height - for example, a z-value of 3.1, corresponding to a p-value of 0.001 - and we only observe the peaks that remain after applying this threshold. This is known as **thresholding** the image, or, more specifically, setting a **cluster-defining threshold.**

https://andysbrainbook.readthedocs.io/en/latest/_images/Zstat_Peak_Demo.gif

After that we need to do simulations in order to determine that the clusters we found are not caused by noise. We do this by creating artificial datasets with the same dimensions and smoothness as our task dataset but which are composed of pure noise. If we do this thousands of times, we can create a distribution of maximum cluster sizes - and from this, we can calculate the percentage of the time we would observe a cluster as large as the one we generated from our task dataset. If that percentage is lower than our alpha level of 5%, we can reject the null hypothesis.

# mri_glmfit-sim

After you have run your general linear model and created group-level contrast maps, you will need to correct for the amount of tests that you have run

Make it into `runClustSims.sh`

```bash
#!/bin/bash

study=$1

for meas in thickness volume; do
  for hemi in lh rh; do
    for smoothness in 10; do
      for dir in Out/${hemi}.${meas}.${study}.${smoothness}.glmdir; do
        mri_glmfit-sim \\
          --glmdir ${dir} \\
          --cache 3.0 pos \\
          --cwp 0.05  \\
          --2spaces
      done
    done
  done
done
```

The arguments specify the following:

1.  The directory that is being corrected for multiple comparisons (`-glmdir`);
2.  The vertex-wise cluster threshold (`-cache`);
3.  The cluster-wise p-threshold (`-cwp`, always set to 0.05 unless you have reasons for doing otherwise);
4.  Correction for analyzing both hemispheres (`-2spaces`)

For most analyses, the `--glmdir`, `--cwp`, and `--2spaces` options will not need to be changed.

The `--cache` option, on the other hand, may be changed depending on your analysis:

-   The first argument to the `--cache`option is the **vertex-wise threshold**
    
    -   Only vertices above this threshold will be considered part of the clusters that will then be tested for statistical significance.
    -   FreeSurfer has pre-computed the number of contiguous vertices at each vertex-wise threshold in order to be labeled a significant cluster.
-   The following table shows you which vertex-wise thresholds have been **cached**, or already stored in memory. (These were generated by using the `--qcache` option during [recon-all](https://andysbrainbook.readthedocs.io/en/latest/FreeSurfer/FS_ShortCourse/FS_03_ReconAll.html#fs-03-reconall))
- 
 | -log10(P) value | p-value |
 | --------------- | ------- |
 | 1.3             | 0.05    |
 | 2.0             | 0.01    |
 | 2.3             | 0.005   |
 | 3.0             | 0.001   |
 | 3.3             | 0.0005  |
 | 4.0             | 0.0001  |

-   The second argument after the `--cache` option specifies the **direction** of the test you are analyzing:
    
    -   The positive direction (`pos`)
    -   The negative direction (`neg`),
    -   Both directions (`abs`)

> If you are using the `--cache` option, it is now recommended to use a value of 3.0 or higher.

A recent paper by [Greve & Fischl (2018)](https://www.sciencedirect.com/science/article/pii/S1053811917310960) demonstrated that using a lower vertex-wise threshold leads to inflated false positives. To maintain a false positive rate of 0.05, either use a vertex-wise threshold of 3.0, or use a permutation test with the `--perm`option. See the help output of `mri_glmfit-sim` for more details.

# Viewing the Results

Navigate to any of the contrast directories. There are several new files that have been created:

**insert img**

-   `cache` indicates that the cluster simulation was performed using cached simulations;
-   `th13` means that a vertex-wise threshold of 1.3 was used; and `pos` indicates the direction of the test.
-   `cluster.summary` indicates a listing of each cluster that was determined to be statistically significant
    -   If significant this is what it would look like in `cluster.summary`
	   ![[../Images/Pasted image 20230220171826.png]]

You can render the clusters on the `fsaverage` template

```bash
overf=$SUBJECTS_DIR/Out/lh.thickness.socecogrp.10.glmdir/NB40-B40/cache.th30.pos.sig.cluster.mgh
freeview -f $SUBJECTS_DIR/fsaverage/surf/lh.inflated:overlay=$overf
```

This is what it would look like given the significant data above

  ![[../Images/Pasted image 20230220171726.png]]

# Higher-Level Script

Instead of running every script manually, you can do this instead

```bash
#!/bin/bash

study=$1

bash runMrisPreproc.sh $study
bash runGLMs.sh $study
bash runClustSims.sh $study
```

Run this

```bash
bash runAllGroupScripts.sh {fsgdfilename}
```