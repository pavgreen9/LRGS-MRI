We probably won't be running freesurfer as a standalone pipeline as its become more commonplace to pre-preprocess your data before inputting into freesurfer or other processes due to increased accuracy ([Glasser et al., (2013)](https://doi.org/10.1016/j.neuroimage.2013.04.127); [Esteban et al. (2019)](https://www.nature.com/articles/s41592-018-0235-4)), though its worth noting how to run it anyways.

You will also need freesurfer regardless as its required for the stat analysis and visualisation.

## Setting Up

### Create a file to store recon-all output

```bash
cd /home/pavgreen/Documents/LRGS/MRI/Participants #your participant folder
mkdir PS
```

### Source freesurfer
```bash
#!/bin/bash
export FREESURFER_HOME=/usr/local/freesurfer/7.3.2 # where freesurfer is installed
source $FREESURFER_HOME/SetUpFreeSurfer.sh
freesurfer
```

### Subject directory
```bash
SUBJECTS_DIR=/home/pavgreen/Documents/LRGS/MRI/Participants/PS
```

### Find the number of logical cores and RAM you have

```bash
# For mac users
sysctl hw.physicalcpu hw.logicalcpu

# For debian based 
nproc
```

### Install parallel

Pre-processing one participant can take 3 hours to days depending on core speed and resolution

However, pre-processing one participant only makes use of one core, parallel allows you to run pre-processing on multiple participants by making use of multiple cores

- Instead of 4 hours per participant, you could do 4 hours every 8 participants (depending on cores)

```bash
sudo apt update
sudo apt install parallel
```

- After testing, `ram` seems to be a bigger bottleneck than the logical cores for systems with less than 16gigs, I recommend the number of participants processed at once to be half or one less than half of your `ram`
    - If 16 then 7 - 8
    - If 8 then 3 - 4
    - If lower than 8 and not on linux limit to 3
    - If more than 16 i.e. 32, logical core is more likely a bottleneck

## Running code for $1mm^3$ image
```bash
find . -type d -name "anat" -exec find {} -name "*.nii" \; | recon-all -s {.} -i {} -all -qcache -3T -parallel --jobs 8 -openmp 12
```

What the code does:

1.  The `find .` command digs through the current directory and all subsequent sub-directories for a directory with the name `anat`
2.  Then it executes another `find` by piping the outputted `anat` directory into it to find a `.nii` file, with the `*` signifying a wildcard
3.  The list of outputted `.nii` directories are then is then piped into the `parallel` command
4. `-parallel --jobs <n>`, where `n` is the number of participants you want to process at once (depending on your thread count) reduces the time from 1 participant every `x hours `to `n` participants every `x hours` 
	1. You can further increase the speed by maxing it out with `-openmp` (if you have spare threads)
5.  The dot in the curly braces for the `-s` option signifies the removal of `.nii` extension; the input to `s` will be 9xxx or Axxx
6.  The `i` option indicates to use the `.nii` output of the `find` command as input to the `parallel` command.
7.  The `all` will run all pre-processing steps on your data. Except for when you are re-running a recon-all command after [editing the data](https://andysbrainbook.readthedocs.io/en/latest/FreeSurfer/FS_ShortCourse/FS_12_FailureModes.html#fs-12-failuremodes), you will always want to use the `-all` option.
8.  The `qcache` which will smooth the data at different levels and store them in the subjectâ€™s output directory. These will be useful for [group level analyses](https://andysbrainbook.readthedocs.io/en/latest/FreeSurfer/FS_ShortCourse/FS_08_GroupAnalysis.html#fs-08-groupanalysis).
    1.  Specifically, it will generate thickness, volume, and curvature maps at several different smoothing sizes, such as 0mm, 10mm, and 25mm full-width half-maximum kernels
    2.  One of the benefits of surface-based analysis is that you can use much larger smoothing kernels than you can in volumetric-based analyses, because there is no risk of smoothing across gyri

## Running code for submm image (< $1mm^3$ )
[`-hires` documentation](https://surfer.nmr.mgh.harvard.edu/fswiki/SubmillimeterRecon)

It is recommended that voxels be between $1mm^3$ and $0.75mm^3$ for smooth running. Was not tested with anisotropic images and images less than $0.5mm^3$ results in brainmask failure - discussed below. 

#### Create your expert.opts file first
```bash
cd <where ever you want the file to be>
echo "mris_inflate -n 100 > expert.opts"
```
This is to ensure sufficient inflation before surface mapping. For $0.75mm$ isotropic voxels, 20-50 iterations work well. You can reinflate your hemispheres later if they're too round.

```bash
mris_inflate -n 15 ?h.smoothwm ?h.inflated
```

### If your image is between $0.75mm^3$ and $1mm^3$
```bash
find . -type d -name "anat" -exec find {} -name "*.nii" \; | \
recon-all \ 
	-autorecon1 \
	-s {.} \
	-i {} \
	-qcache \
	-3T \
	-hires \
	-expert <insert expert file here> \
	-parallel \
	-openmp 12
```

### If your image is < $0.75mm^3$
#### I recommend doing `recon-all` in stages

Documentation also mentioned that if the brainmask were to be fixed manually i.e. do a skullstrip prior to freesurfer and input as file, it should work . 

However, it is important to note that `recon-all` on a $1mm^3$ resolution is already quite intensive (3-5 hours) for one participant - without `-parallel`.  By increasing the resolution, the time and computing costs increases quite a bit. 

For reference, one `0.48mm x 0.48mm x 1mm`  image, which was isometricised by freesurfer to be $0.1mm^3$ took me about 12 hours to *almost* finish running with `-parallel -openmp 12`  and 16 GBs of RAM, before it crashed. 

#### `-autorecon1`
```bash
ind . -type d -name "anat" -exec find {} -name "*.nii" \; | recon-all \
-autorecon1 \
-s {.} \
-i {} \
-qcache \
-3T \
-hires \
-parallel \
-openmp 12
```

#### `-autorecon2`
```bash
ind . -type d -name "anat" -exec find {} -name "*.nii" \; | recon-all \
-autorecon2 \
-s test2 \
-qcache \
-3T \
-hires \
-expert <insert expert file here> \
-parallel \
-openmp 12
```

#### `-autorecon3`
```bash
ind . -type d -name "anat" -exec find {} -name "*.nii" \; | recon-all \
-autorecon3 \
-s test2 \
-qcache \
-3T \
-hires \
-expert <insert expert file here> \
-parallel \
-openmp 12
```

### Errors
1. If you get a permission error when running recon-all, type the following: 
```bash
sudo chmod -R a+w $SUBJECTS_DIR And then rerun the recon-all command
```

2. If you get an error `Cannot find rh.white.H` it might be due to `-parallel` so remove that for participants with failures. [Source](https://afni.nimh.nih.gov/pub/dist/doc/htmldoc/tutorials/fs/fs_fsprep.html)

# Outputs
From here on out, all images will be of A697.

## Orig freesurfer Output
**Original** output with freesurfer doing automatic downsampling and isometric resampling to 1mm.
![[../../Images/Pasted image 20230226153132.png]]
![[../../Images/Pasted image 20230226152455.png]]

## SynthStrip -> freesurfer 1mm
**SynthStrip -> freesurfer** with freesurfer doing automatic downsampling and isometric resampling to 1mm.

## SynthStrip -> Nibabel-downsample -> Freesurfer 1mm
**SynthStrip -> nibabel-downsampling -> freesurfer** at 1mm.

## SynthStrip -> Nibabel-downsample -> Freesurfer 0.7mm
**SynthStrip -> nibabel-downsampling -> freesurfer** at 0.7mm.

## SynthStrip -> freesurfer `-hires` 0.468mm
**SynthStrip -> freesurfer** with freesurfer doing automatic isometric sampling at 0.468mm.

![[../../Images/Pasted image 20230226153150.png]]
![[../../Images/Pasted image 20230226152336.png]]

## Notes on Output

### Resolution
- Resolution differences is definitely noticeable, with 0.468mm looking much clearer

### Brainmask
- **Original** **1mm** output has dura remaining - not clean 
- **SynthStripped->freesurfer `-hires`** at **.468mm** eats into the occipital - even though the original synthstrip output is actually very good
	- The reason for this lies in the aggressive skull removal eating into the occipital which can be circumvented by using a lower watershed level - refer to [[4.2 Fixing Errors]]

### Surface-based Segmentation
- **Original** **1mm** is more lenient with its segmentation - owing to the lower resolution 
- **SynthStripped->freesurfer `-hires`** at **.468mm**  - more strict with its segmentation 

### Performance
1. **Original** **1mm** output takes 5 hours every 8 participant
2. **SynthStripped->Freesurfer `-hires`** at **.468mm** takes 12 hours per participant and is computationally intensive - my computer crashed towards the end 

## Future Considerations
- Due to the insane number of permutations - I've decided to narrow down the sequence to skullstrip -> preprocess as literature often highlights its effectiveness
1. Might be worth considering running the SynthStrip output at 1mm and 0.7mm - assuming SynthStrip
	1. At 1mm we could compare
		2. **SynthStrip -> freesurfer** 
		3. **SynthStrip -> nibabel-downsampling -> freesurfer**
	3. At 0.7mm we could compare
		2. **SynthStrip -> nibabel-downsample -> freesurfer `-hires`** 